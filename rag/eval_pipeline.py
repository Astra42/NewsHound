"""
–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ü–µ–Ω–∫–∏ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DeepEval –º–µ—Ç—Ä–∏–∫.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è ground truth.
–ú–µ—Ç—Ä–∏–∫–∏:
- AnswerRelevancyMetric: –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
- ContextualPrecisionMetric: –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ retrieval
"""

import asyncio
import json
import os
import pickle
import random
import time
from datetime import datetime
from typing import Any, Dict, List

import config as cfg
import httpx
from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.test_case import LLMTestCase
from langchain_mistralai import ChatMistralAI
from prompts import QUESTION_GEN_PROMPT
from retrieval import get_qdrant_client, retrieve_context
from vectorstore import VectorStore


class MistralJudgeModel(DeepEvalBaseLLM):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è Mistral –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ DeepEval."""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or cfg.JUDGE_MODEL
        self.model = ChatMistralAI(
            model=self.model_name,
            api_key=cfg.MISTRAL_API_KEY,
            max_retries=5,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
            timeout=60.0,  # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ 60 —Å–µ–∫—É–Ω–¥
        )

    def load_model(self):
        return self.model

    def generate(self, prompt: str) -> str:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.model.invoke(prompt)
                return response.content
            except (
                httpx.RemoteProtocolError,
                httpx.ConnectError,
                httpx.TimeoutException,
            ) as e:
                if attempt < max_retries - 1:
                    wait_time = 2**attempt
                    time.sleep(wait_time)
                else:
                    return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ judge: {type(e).__name__}]"
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ judge: {type(e).__name__}]"

        return "[–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç judge]"

    async def a_generate(self, prompt: str) -> str:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = await self.model.ainvoke(prompt)
                return response.content
            except (
                httpx.RemoteProtocolError,
                httpx.ConnectError,
                httpx.TimeoutException,
            ) as e:
                if attempt < max_retries - 1:
                    wait_time = 2**attempt
                    await asyncio.sleep(wait_time)
                else:
                    return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ judge: {type(e).__name__}]"
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ judge: {type(e).__name__}]"

        return "[–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç judge]"

    def get_model_name(self) -> str:
        return self.model_name


def generate_answer(llm, question: str, context: list[str], max_retries: int = 3) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.
    
    Args:
        llm: –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        question: –≤–æ–ø—Ä–æ—Å
        context: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        max_retries: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        
    Returns:
        —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    """
    context_str = "\n".join(context)
    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_str}\n\n–í–æ–ø—Ä–æ—Å: {question}\n\n–û—Ç–≤–µ—Ç:"
    
    for attempt in range(max_retries):
        try:
            response = llm.invoke(prompt)
            return response.content
        except (httpx.RemoteProtocolError, httpx.ConnectError, httpx.TimeoutException, ConnectionError) as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: 1s, 2s, 4s
                print(f"    –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time}—Å... ({type(e).__name__})")
                time.sleep(wait_time)
            else:
                print(f"    –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã, –æ—à–∏–±–∫–∞: {e}")
                return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {type(e).__name__}]"
        except Exception as e:
            print(f"    –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {type(e).__name__}]"
    
    return "[–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç]"


def load_test_posts(filepath: str = None) -> List[Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ pickle —Ñ–∞–π–ª–∞.

    Args:
        filepath: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É test_posts.pkl

    Returns:
        —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {"content": str, "metadata": dict}
    """
    filepath = filepath or cfg.TEST_POSTS_FILE

    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏
    possible_paths = [
        filepath,
        os.path.join("rag", filepath),
        os.path.join("test_data", "test_posts.pkl"),
        os.path.join("..", "test_data", "test_posts.pkl"),
    ]

    for path in possible_paths:
        if os.path.exists(path):
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ {path}...")
            with open(path, "rb") as f:
                posts = pickle.load(f)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            if isinstance(posts, list):
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                formatted_posts = []
                for post in posts:
                    if isinstance(post, dict):
                        formatted_posts.append(
                            {
                                "content": post.get("text", post.get("content", "")),
                                "metadata": {
                                    "source": "telegram",
                                    "channel": post.get("channel", ""),
                                    "date": post.get("date", ""),
                                    "id": post.get("id", ""),
                                },
                            }
                        )
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(formatted_posts)} –ø–æ—Å—Ç–æ–≤")
                return formatted_posts
            else:
                print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –≤ {path}")
                return []

    print(f"–§–∞–π–ª test_posts.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç—è–º: {possible_paths}")
    return []


def generate_ground_truth_documents(
    vectorstore: VectorStore, source_document: str, k: int = None
) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ground truth –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞.

    Ground truth –≤–∫–ª—é—á–∞–µ—Ç:
    1. –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (source_document)
    2. –ü–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ semantic search

    Args:
        vectorstore: VectorStore –¥–ª—è –ø–æ–∏—Å–∫–∞
        source_document: –∏—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞

    Returns:
        —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (ground truth)
    """
    k = k or cfg.GROUND_TRUTH_SIMILAR_DOCS

    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ –∑–∞–ø—Ä–æ—Å)
    similar_docs_result = vectorstore.search(source_document, k=k + 1)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    similar_docs = [
        doc["content"] if isinstance(doc, dict) else doc for doc in similar_docs_result
    ]

    # –°–æ–±–∏—Ä–∞–µ–º ground truth: source + –ø–æ—Ö–æ–∂–∏–µ (–∏—Å–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)
    ground_truth = [source_document]

    for content in similar_docs:
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å–∞–º source document –∏ –µ—â–µ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ
        if content != source_document and content not in ground_truth:
            ground_truth.append(content)
            if len(ground_truth) >= k + 1:  # +1 –¥–ª—è source
                break

    return ground_truth


def generate_test_cases_with_ground_truth(
    vectorstore: VectorStore, test_posts: List[Dict[str, Any]], num_cases: int = 10
) -> List[Dict[str, Any]]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å ground truth –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤.

    Args:
        vectorstore: VectorStore —Å –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        test_posts: —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
        num_cases: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤

    Returns:
        —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å ground truth
    """
    print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_cases} —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å ground truth...")

    # LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
    llm = ChatMistralAI(
        model=cfg.JUDGE_MODEL, api_key=cfg.MISTRAL_API_KEY, max_retries=5, timeout=60.0
    )

    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ—Å—Ç—ã
    selected_posts = random.sample(test_posts, min(num_cases, len(test_posts)))

    test_cases = []
    for i, post in enumerate(selected_posts, 1):
        print(f"[{i}/{len(selected_posts)}] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç-–∫–µ–π—Å–∞...")

        source_content = post["content"]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        prompt = QUESTION_GEN_PROMPT.format(document=source_content)

        try:
            response = llm.invoke(prompt)
            question = response.content.strip().strip("\"'")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ground truth –¥–æ–∫—É–º–µ–Ω—Ç—ã
            ground_truth_docs = generate_ground_truth_documents(
                vectorstore, source_content, k=cfg.GROUND_TRUTH_SIMILAR_DOCS
            )

            test_case = {
                "question": question,
                "source_document": source_content,
                "ground_truth_documents": ground_truth_docs,
                "metadata": post.get("metadata", {}),
            }

            test_cases.append(test_case)
            print(f"    –í–æ–ø—Ä–æ—Å: {question[:60]}...")
            print(f"    Ground truth –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(ground_truth_docs)}")

        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞: {e}")
            continue

    print(f"\n–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤")
    return test_cases


def evaluate_with_deepeval(
    vectorstore: VectorStore,
    test_cases: List[Dict[str, Any]],
    llm_generation: ChatMistralAI,
    judge_model: MistralJudgeModel,
) -> List[Dict[str, Any]]:
    """
    –û—Ü–µ–Ω–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DeepEval –º–µ—Ç—Ä–∏–∫.

    Args:
        vectorstore: VectorStore –¥–ª—è retrieval
        test_cases: —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å ground truth
        llm_generation: LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
        judge_model: LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (Judge)

    Returns:
        —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏
    """
    print(f"\n–û—Ü–µ–Ω–∫–∞ {len(test_cases)} —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å DeepEval –º–µ—Ç—Ä–∏–∫–∞–º–∏...\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
    answer_relevancy_metric = AnswerRelevancyMetric(
        threshold=cfg.ANSWER_RELEVANCY_THRESHOLD, model=judge_model, include_reason=True
    )

    contextual_precision_metric = ContextualPrecisionMetric(
        threshold=cfg.CONTEXTUAL_PRECISION_THRESHOLD,
        model=judge_model,
        include_reason=True,
    )

    results = []

    for i, test_case in enumerate(test_cases, 1):
        question = test_case["question"]
        ground_truth_docs = test_case["ground_truth_documents"]

        print(f"[{i}/{len(test_cases)}] {question[:50]}...")

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ API
        if i > 1:
            time.sleep(2)  # 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        # 1. Retrieval
        retrieved_contexts = retrieve_context(vectorstore, question)

        # 2. Generation (—Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫)
        try:
            answer = generate_answer(
                llm_generation, question, retrieved_contexts, max_retries=3
            )
        except Exception as e:
            print(f"    –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            answer = f"[–û—à–∏–±–∫–∞: {type(e).__name__}]"

        # 3. –û—Ü–µ–Ω–∫–∞ AnswerRelevancy
        test_case_ar = LLMTestCase(
            input=question,
            actual_output=answer,
            retrieval_context=retrieved_contexts,
            expected_output=None,
        )

        try:
            answer_relevancy_metric.measure(test_case_ar)
            answer_relevancy_score = answer_relevancy_metric.score
            answer_relevancy_passed = answer_relevancy_metric.is_successful()
            answer_relevancy_reason = getattr(answer_relevancy_metric, "reason", None)
        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ AnswerRelevancy: {e}")
            answer_relevancy_score = None
            answer_relevancy_passed = False
            answer_relevancy_reason = str(e)

        # 4. –û—Ü–µ–Ω–∫–∞ ContextualPrecision
        test_case_cp = LLMTestCase(
            input=question,
            actual_output=answer,
            retrieval_context=retrieved_contexts,
            expected_output=ground_truth_docs,
        )

        try:
            contextual_precision_metric.measure(test_case_cp)
            contextual_precision_score = contextual_precision_metric.score
            contextual_precision_passed = contextual_precision_metric.is_successful()
            contextual_precision_reason = getattr(
                contextual_precision_metric, "reason", None
            )
        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ ContextualPrecision: {e}")
            contextual_precision_score = None
            contextual_precision_passed = False
            contextual_precision_reason = str(e)

        result = {
            "question": question,
            "source_document": test_case["source_document"],
            "ground_truth_documents": ground_truth_docs,
            "retrieved_documents": retrieved_contexts,
            "answer": answer,
            "scores": {
                "answer_relevancy": {
                    "score": answer_relevancy_score,
                    "passed": answer_relevancy_passed,
                    "reason": answer_relevancy_reason,
                },
                "contextual_precision": {
                    "score": contextual_precision_score,
                    "passed": contextual_precision_passed,
                    "reason": contextual_precision_reason,
                },
            },
        }

        results.append(result)

        print(
            f"    AnswerRelevancy: {answer_relevancy_score:.3f} {'‚úì' if answer_relevancy_passed else '‚úó'}"
        )
        print(
            f"    ContextualPrecision: {contextual_precision_score:.3f} {'‚úì' if contextual_precision_passed else '‚úó'}"
        )

    return results


def aggregate_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏."""
    answer_relevancy_scores = [
        r["scores"]["answer_relevancy"]["score"]
        for r in results
        if r["scores"]["answer_relevancy"]["score"] is not None
    ]

    contextual_precision_scores = [
        r["scores"]["contextual_precision"]["score"]
        for r in results
        if r["scores"]["contextual_precision"]["score"] is not None
    ]

    answer_relevancy_passed = sum(
        1 for r in results if r["scores"]["answer_relevancy"].get("passed", False)
    )

    contextual_precision_passed = sum(
        1 for r in results if r["scores"]["contextual_precision"].get("passed", False)
    )

    summary = {
        "answer_relevancy": {
            "average_score": sum(answer_relevancy_scores) / len(answer_relevancy_scores)
            if answer_relevancy_scores
            else None,
            "min_score": min(answer_relevancy_scores)
            if answer_relevancy_scores
            else None,
            "max_score": max(answer_relevancy_scores)
            if answer_relevancy_scores
            else None,
            "pass_rate": answer_relevancy_passed / len(results) if results else 0,
            "passed": answer_relevancy_passed,
            "total": len(results),
        },
        "contextual_precision": {
            "average_score": sum(contextual_precision_scores)
            / len(contextual_precision_scores)
            if contextual_precision_scores
            else None,
            "min_score": min(contextual_precision_scores)
            if contextual_precision_scores
            else None,
            "max_score": max(contextual_precision_scores)
            if contextual_precision_scores
            else None,
            "pass_rate": contextual_precision_passed / len(results) if results else 0,
            "passed": contextual_precision_passed,
            "total": len(results),
        },
    }

    return summary


def print_detailed_summary(summary: Dict[str, Any], results: List[Dict[str, Any]]):
    """–í—ã–≤–æ–¥ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
    print("\n" + "=" * 70)
    print("–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –û–¶–ï–ù–ö–ï RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 70)

    # Answer Relevancy
    ar = summary["answer_relevancy"]
    print("\nüìä Answer Relevancy (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞):")
    print(
        f"  –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {ar['average_score']:.3f}"
        if ar["average_score"]
        else "  –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: N/A"
    )
    print(
        f"  –ú–∏–Ω/–ú–∞–∫—Å: {ar['min_score']:.3f} / {ar['max_score']:.3f}"
        if ar["min_score"]
        else "  –ú–∏–Ω/–ú–∞–∫—Å: N/A"
    )
    print(f"  Pass Rate: {ar['pass_rate'] * 100:.1f}% ({ar['passed']}/{ar['total']})")
    print(f"  –ü–æ—Ä–æ–≥: {cfg.ANSWER_RELEVANCY_THRESHOLD}")

    cp = summary["contextual_precision"]
    print("\nüìä Contextual Precision (–¢–æ—á–Ω–æ—Å—Ç—å Retrieval):")
    print(
        f"  –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {cp['average_score']:.3f}"
        if cp["average_score"]
        else "  –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: N/A"
    )
    print(
        f"  –ú–∏–Ω/–ú–∞–∫—Å: {cp['min_score']:.3f} / {cp['max_score']:.3f}"
        if cp["min_score"]
        else "  –ú–∏–Ω/–ú–∞–∫—Å: N/A"
    )
    print(f"  Pass Rate: {cp['pass_rate'] * 100:.1f}% ({cp['passed']}/{cp['total']})")
    print(f"  –ü–æ—Ä–æ–≥: {cfg.CONTEXTUAL_PRECISION_THRESHOLD}")

    failed_cases = [
        r
        for r in results
        if not r["scores"]["answer_relevancy"].get("passed", True)
        or not r["scores"]["contextual_precision"].get("passed", True)
    ]

    if failed_cases:
        print(f"\n‚ö†Ô∏è  –ù–µ—É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤: {len(failed_cases)}")
        print("\n–ü—Ä–∏–º–µ—Ä—ã –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤:")
        for i, case in enumerate(failed_cases[:3], 1):
            print(f"\n  {i}. –í–æ–ø—Ä–æ—Å: {case['question'][:60]}...")
            if not case["scores"]["answer_relevancy"]["passed"]:
                print(
                    f"     AnswerRelevancy: {case['scores']['answer_relevancy']['score']:.3f}"
                )
            if not case["scores"]["contextual_precision"]["passed"]:
                print(
                    f"     ContextualPrecision: {case['scores']['contextual_precision']['score']:.3f}"
                )

    print("\n" + "=" * 70)


def save_detailed_report(
    results: List[Dict[str, Any]], summary: Dict[str, Any], filepath: str = None
):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
    filepath = (
        filepath or f"full_eval_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )

    report = {
        "timestamp": datetime.now().isoformat(),
        "num_test_cases": len(results),
        "config": {
            "judge_model": cfg.JUDGE_MODEL,
            "llm_model": cfg.LLM_MODEL,
            "embedding_model": cfg.EMBEDDING_MODEL,
            "retrieval_k": cfg.RETRIEVAL_K,
            "answer_relevancy_threshold": cfg.ANSWER_RELEVANCY_THRESHOLD,
            "contextual_precision_threshold": cfg.CONTEXTUAL_PRECISION_THRESHOLD,
        },
        "summary": summary,
        "detailed_results": results,
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")


def run_full_evaluation(
    vectorstore: VectorStore = None,
    test_posts: List[Dict[str, Any]] = None,
    num_test_cases: int = 10,
    save_report: bool = True,
) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ—Ü–µ–Ω–∫–∏ RAG —Å–∏—Å—Ç–µ–º—ã.

    Args:
        vectorstore: VectorStore (–µ—Å–ª–∏ None - –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Qdrant)
        test_posts: —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ (–µ—Å–ª–∏ None - –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ —Ñ–∞–π–ª–∞)
        num_test_cases: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        save_report: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –æ—Ç—á–µ—Ç

    Returns:
        —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
    """
    print("=" * 70)
    print("–ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –û–¶–ï–ù–ö–ò RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 70)

    # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ vectorstore
    if vectorstore is None:
        vectorstore = get_qdrant_client()
        if vectorstore is None:
            return {}

    info = vectorstore.get_collection_info()
    print(f"\n–ö–æ–ª–ª–µ–∫—Ü–∏—è: {info}")

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
    if test_posts is None:
        test_posts = load_test_posts()
        if not test_posts:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞.")
            print("–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Qdrant...")

            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –ø–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–ø—Ä—è–º—É—é –∏–∑ Qdrant
            try:
                scroll_result = vectorstore.client.scroll(
                    collection_name=vectorstore.collection_name,
                    limit=num_test_cases * 2,
                    with_payload=True,
                    with_vectors=False,
                )
                points = (
                    scroll_result[0]
                    if isinstance(scroll_result, tuple)
                    else scroll_result
                )
                test_posts = [
                    {
                        "content": p.payload.get("content", "") if p.payload else "",
                        "metadata": {
                            k: v
                            for k, v in (p.payload.items() if p.payload else {})
                            if k != "content"
                        },
                    }
                    for p in points
                ]
                print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(test_posts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Qdrant")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Qdrant: {e}")
                return {}

    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤ —Å ground truth
    test_cases = generate_test_cases_with_ground_truth(
        vectorstore, test_posts, num_cases=num_test_cases
    )

    if not test_cases:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç-–∫–µ–π—Å—ã")
        return {}

    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    llm_generation = ChatMistralAI(
        model=cfg.LLM_MODEL, api_key=cfg.MISTRAL_API_KEY, max_retries=5, timeout=60.0
    )

    judge_model = MistralJudgeModel()

    # 5. –û—Ü–µ–Ω–∫–∞ —Å DeepEval –º–µ—Ç—Ä–∏–∫–∞–º–∏
    results = evaluate_with_deepeval(
        vectorstore, test_cases, llm_generation, judge_model
    )

    # 6. –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    summary = aggregate_results(results)

    # 7. –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print_detailed_summary(summary, results)

    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    if save_report:
        save_detailed_report(results, summary)

    return {"summary": summary, "results": results}


def main():
    """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ—Ü–µ–Ω–∫–∏."""
    results = run_full_evaluation(num_test_cases=10)
    return results


if __name__ == "__main__":
    main()
